{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP_LAB2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyPV7+i+1+865OBew3yv7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VyomaD/kdm_tech/blob/main/ICP_lab2/source/ICP_LAB2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErcaiKzJ6Wfv",
        "outputId": "450e2bdb-d1ee-4daa-da6d-09c2b96fd516"
      },
      "source": [
        "# Installing all nltk libraries and Spacy, neuralcoref\n",
        "# Install nltk and then Spacy and then restart the runtime and then continue\n",
        "# !pip install nltk\n",
        "# !pip install spacy==2.1.0\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download en\n",
        "# !pip install neuralcoref\n",
        "import nltk\n",
        "import spacy\n",
        "import neuralcoref\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk import pos_tag, word_tokenize, ChartParser\n",
        "from nltk import sent_tokenize\n",
        "from nltk.draw.tree import draw_trees\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "neuralcoref.add_to_pipe(nlp)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f39d618d898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1OYS7wt7hWG"
      },
      "source": [
        "# Lemmatize the sentence using WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZBsOUr-7sU8"
      },
      "source": [
        "# defining a function called as process_sentence with sent as parameter as input\n",
        "def process_sentence(sent):\n",
        "    r\"\"\"Processing a sentence to extract POS, lemma and NER for each word in \n",
        "    the sentence\n",
        "    \n",
        "    Args:\n",
        "        sent: input sentence as a string\n",
        "    \n",
        "    Return:\n",
        "        Returns a tuple of dictionaries for POS, lemma and NER\n",
        "    \"\"\"\n",
        "\n",
        "    # initializing nltk word tokenizer library inside a variable word_tokens\n",
        "    word_tokens = nltk.word_tokenize(sent)\n",
        "\n",
        "    # finding pos of each word\n",
        "    # used pos_tags to find all pos inside words\n",
        "    pos_dict = dict(pos_tag(word_tokens))\n",
        "\n",
        "    # finding lemma of each word\n",
        "    # creating empty dict\n",
        "    lemma_dict = {}\n",
        "    # iterating each key and converting first letter of pos into lower case\n",
        "    for word in word_tokens:\n",
        "        # replacing 'j' to 'a' as lemmatizer accepts 'a' as adjective and \n",
        "        # nltk gives 'j' as adjective\n",
        "        w_tag = pos_dict[word][0].lower().replace('j', 'a')\n",
        "  \n",
        "        # finding for each letter \n",
        "        # if first letter is present inside word tag then perform operation\n",
        "        w_tag = w_tag if w_tag in ['a', 'r', 'n', 'v'] else None\n",
        "        # if it is not in word tag then return the word itself\n",
        "        if not w_tag:\n",
        "            lemma_dict[word] = word\n",
        "        else:\n",
        "          # return lemma words\n",
        "            lemma_dict[word] = lemmatizer.lemmatize(word, w_tag)\n",
        "\n",
        "    # finding ner of each word\n",
        "    doc = nlp(sent)\n",
        "    # creating ner group for and defining key value based on text and label \n",
        "    ner_groups = [(X.text, X.label_) for X in doc.ents]\n",
        "    # iterating each word and finding presence of Named entity relationship\n",
        "    ner_dict = {}\n",
        "    for word in word_tokens:\n",
        "        for group in ner_groups:\n",
        "            if word in group[0]:\n",
        "            #if the word is present in groups then return named entity value\n",
        "                ner_dict[word] = group[1]\n",
        "        if word not in ner_dict.keys():\n",
        "            #if not present then return 0\n",
        "            ner_dict[word] = 0\n",
        "\n",
        "    # returning all the values in dictonary (pos, lemma and named entity pair)\n",
        "    return (pos_dict, lemma_dict, ner_dict)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-YCs_EI-Yna"
      },
      "source": [
        "\n",
        "# main program\n",
        "# delivering the sentence using sent tokenize\n",
        "text = \"Xi Jinping is a Chinese politician who has served as General Secretary of the Chinese Communist Party (CCP) and Chairman of the Central Military Commission (CMC) since 2012, and President of the People's Republic of China (PRC) since 2013. He has been the paramount leader of China, the most prominent political leader in the country, since 2012. The son of Chinese Communist veteran Xi Zhongxun, he was exiled to rural Yanchuan County as a teenager following his father's purge during the Cultural Revolution and lived in a cave in the village of Liangjiahe, where he joined the CCP and worked as the party secretary.\"\n",
        "print(text)\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(text)\n",
        "\n",
        "# print format table\n",
        "print(\"{:12s}\\t{:12s}\\t{:6s}\\t{}\".format(\"Word\", \"Lemma\", \"POS\", \"NER\"))\n",
        "# iterating each sentence with indexing \n",
        "for i, sent in enumerate(sent_tokens):\n",
        "    print(\"[Sentence {}]\".format(i+1))\n",
        "    pos_dict, lemma_dict, ner_dict = process_sentence(sent)\n",
        "# retrieving key value from dict\n",
        "    keys = pos_dict.keys()\n",
        "    for key in keys:\n",
        "        # break\n",
        "        print(\"{:12s}\\t{:12s}\\t{:6s}\\t{}\".format(key, lemma_dict[key],\n",
        "              pos_dict[key], ner_dict[key]))\n",
        "    print(\"\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NMlrberB4Ee",
        "outputId": "50a87874-2a4c-4e04-b1c7-e2339bdbb7c9"
      },
      "source": [
        "# parsing\n",
        "# Chunk the pattern for parsing noun phrase using Regexp\n",
        "# This rule says that an NP chunk should be formed whenever the chunker \n",
        "# finds an optional determiner (DT) followed by any number of adjectives (JJ) \n",
        "#and then a noun (NN).\n",
        "\n",
        "# visualising three different types of grammar rules for parsin on three \n",
        "# different sentences to form a syntactic tree\n",
        "\n",
        "pattern1 = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "pattern2 = 'NP: {<NN.?>*<VBD.?>*<JJ.?>*<CC>?}'\n",
        "pattern3 = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
        "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
        "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
        "  \"\"\"\n",
        "\n",
        "# assigning pattern 1 in parser\n",
        "cpharse1 = nltk.RegexpParser(pattern1)\n",
        "# assigning pattern 2 in parser\n",
        "cphrase2 = nltk.RegexpParser(pattern2)\n",
        "# assigning pattern 3 in parser\n",
        "cpharse3 = nltk.RegexpParser(pattern3)\n",
        "\n",
        "print(f\"Parsing sentence 1 using grammar rule: {pattern1}\")\n",
        "# get the first sentence\n",
        "firstsent = sent_tokens[0]\n",
        "firstsentparsed = process_sentence(firstsent)[0]\n",
        "firstsentparsed = list(firstsentparsed.items())\n",
        "#parsing processed sentence using regexp patterns\n",
        "firstcs = cpharse1.parse(firstsentparsed)\n",
        "print(firstcs)\n",
        "print(\"\")\n",
        "\n",
        "print(f\"Parsing sentence 2 using grammar rule: {pattern2}\")\n",
        "# get the second sentence\n",
        "secondsent = sent_tokens[1]\n",
        "secondsentparsed = process_sentence(secondsent)[0]\n",
        "secondsentparsed = list(secondsentparsed.items())\n",
        "#parsing processed sentence using regexp patterns\n",
        "secondcs = cphrase2.parse(secondsentparsed)\n",
        "print(secondcs)\n",
        "print(\"\")\n",
        "\n",
        "print(f\"Parsing sentence 3 using grammar rule: {pattern3}\")\n",
        "# get the third sentence\n",
        "thirdsent = sent_tokens[2]\n",
        "thirdsentparsed = process_sentence(thirdsent)[0]\n",
        "thirdsentparsed = list(thirdsentparsed.items())\n",
        "#parsing processed sentence using regexp patterns\n",
        "thirdcs = cpharse3.parse(thirdsentparsed)\n",
        "print(thirdcs)\n",
        "print(\"\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentence 1 using grammar rule: NP: {<DT>?<JJ>*<NN>}\n",
            "(S\n",
            "  (NP Xi/NN)\n",
            "  Jinping/NNP\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  Chinese/NNP\n",
            "  (NP politician/NN)\n",
            "  who/WP\n",
            "  has/VBZ\n",
            "  served/VBN\n",
            "  as/IN\n",
            "  General/NNP\n",
            "  Secretary/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  Communist/NNP\n",
            "  Party/NNP\n",
            "  (/(\n",
            "  CCP/NNP\n",
            "  )/)\n",
            "  and/CC\n",
            "  Chairman/NNP\n",
            "  Central/NNP\n",
            "  Military/NNP\n",
            "  Commission/NNP\n",
            "  CMC/NNP\n",
            "  since/IN\n",
            "  2012/CD\n",
            "  ,/,\n",
            "  President/NNP\n",
            "  People/NNP\n",
            "  's/POS\n",
            "  Republic/NNP\n",
            "  China/NNP\n",
            "  PRC/NNP\n",
            "  2013/CD\n",
            "  ./.)\n",
            "\n",
            "Parsing sentence 2 using grammar rule: NP: {<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\n",
            "(S\n",
            "  He/PRP\n",
            "  has/VBZ\n",
            "  been/VBN\n",
            "  the/DT\n",
            "  (NP paramount/JJ)\n",
            "  (NP leader/NN)\n",
            "  of/IN\n",
            "  (NP China/NNP)\n",
            "  ,/,\n",
            "  most/RBS\n",
            "  (NP prominent/JJ political/JJ)\n",
            "  in/IN\n",
            "  (NP country/NN)\n",
            "  since/IN\n",
            "  2012/CD\n",
            "  ./.)\n",
            "\n",
            "Parsing sentence 3 using grammar rule: \n",
            "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
            "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
            "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
            "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
            "  \n",
            "(S\n",
            "  (NP The/DT son/NN)\n",
            "  (PP\n",
            "    of/IN\n",
            "    (NP Chinese/JJ Communist/NNP veteran/NN Xi/NNP Zhongxun/NNP))\n",
            "  ,/,\n",
            "  he/PRP\n",
            "  was/VBD\n",
            "  exiled/VBN\n",
            "  to/TO\n",
            "  (NP rural/JJ Yanchuan/NNP County/NNP)\n",
            "  (PP as/IN (NP a/DT teenager/NN))\n",
            "  following/VBG\n",
            "  his/PRP$\n",
            "  (NP father/NN)\n",
            "  's/POS\n",
            "  (NP purge/NN)\n",
            "  (PP during/IN (NP the/DT Cultural/JJ Revolution/NNP))\n",
            "  and/CC\n",
            "  lived/VBN\n",
            "  (PP in/IN (NP cave/NN village/NN Liangjiahe/NNP))\n",
            "  where/WRB\n",
            "  joined/VBD\n",
            "  (NP CCP/NNP)\n",
            "  worked/VBD\n",
            "  (NP party/NN secretary/NN)\n",
            "  ./.)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OMZUcQ6CTq5",
        "outputId": "8eb4877e-209b-455a-e6a3-b84273805a2c"
      },
      "source": [
        "# co-coref resolution\n",
        "co_coref_tokens = nlp(text)\n",
        "\n",
        "def coref_mentions(co_coref_tokens):\n",
        "    r\"\"\"Extracting all mentions in the given text\n",
        "    \n",
        "    Args:\n",
        "        co_coref_tokens: input text parsed\n",
        "    \n",
        "    Return:\n",
        "        Returns the list of all coreferences from the sentence\n",
        "    \"\"\"\n",
        "\n",
        "    print('\\nAll the \"mentions\" in the given text:')\n",
        "    for cluster in co_coref_tokens._.coref_clusters:\n",
        "        print(cluster.mentions)\n",
        "\n",
        "if co_coref_tokens._.has_coref:\n",
        "    print(\"Given text: \" + text)\n",
        "    coref_mentions(co_coref_tokens)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given text: Xi Jinping is a Chinese politician who has served as General Secretary of the Chinese Communist Party (CCP) and Chairman of the Central Military Commission (CMC) since 2012, and President of the People's Republic of China (PRC) since 2013. He has been the paramount leader of China, the most prominent political leader in the country, since 2012. The son of Chinese Communist veteran Xi Zhongxun, he was exiled to rural Yanchuan County as a teenager following his father's purge during the Cultural Revolution and lived in a cave in the village of Liangjiahe, where he joined the CCP and worked as the party secretary.\n",
            "\n",
            "All the \"mentions\" in the given text:\n",
            "[Xi Jinping, He, he, his, he]\n",
            "[China, China, the country]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}