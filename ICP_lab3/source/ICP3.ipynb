{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0zC3bUAAU1q+MYQCjZ1kQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VyomaD/kdm_tech/blob/main/ICP_lab3/source/ICP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNLMU1M60z3N",
        "outputId": "a2c97421-66bf-4fba-998f-dcafb39cebc4"
      },
      "source": [
        "# Installing all nltk libraries and Spacy, textacy\n",
        "# Install nltk and then Spacy and then restart the runtime and then continue\n",
        "# !pip install nltk\n",
        "# !pip install spacy==2.1.0\n",
        "# !python -m spacy download en\n",
        "# !pip install textacy\n",
        "# import all the dependencies \n",
        "import nltk\n",
        "import spacy\n",
        "import textacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import sent_tokenize\n",
        "nlp = spacy.load('en')\n",
        "\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RTzC8L1YCsO",
        "outputId": "00f29bec-315b-48b1-b87f-863ff5d7d61a"
      },
      "source": [
        "# ----------------- TASK 1 ------------------\n",
        "\n",
        "# Performing Triplets Extraction based on Input Sentence\n",
        "\n",
        "SUBJ = [\"nsubj\",\"nsubjpass\"]\n",
        "VERB = [\"ROOT\"]\n",
        "OBJ = [\"dobj\", \"pobj\", \"dobj\"]\n",
        "text = nlp(u\"Twenty percent electric motors are pulled from an assembly line\")\n",
        "text1 = nlp(u\"Mary had a little lamb. It's fleece was white as snow\")\n",
        "str(text)\n",
        "sentences = nltk.sent_tokenize(str(text))\n",
        "\n",
        "len(sentences)\n",
        "\n",
        "# Defining triplet empty list and iterating each sentence to find subj tokens,\n",
        "# object tokens and their relations.\n",
        "\n",
        "triples = []\n",
        "for sentence in sentences:\n",
        "    # break\n",
        "    text = nlp(sentence)\n",
        "    sub_toks = [tok for tok in text if (tok.dep_ in SUBJ)]\n",
        "    sub_toks = [tok for tok in sub_toks if str(tok).isalnum()]\n",
        "    obj_toks = [tok for tok in text if (tok.dep_ in OBJ)]\n",
        "    obj_toks = [tok for tok in obj_toks if str(tok).isalnum()]\n",
        "    vrb_toks = [tok for tok in text if (tok.dep_ in VERB)]\n",
        "    vrb_toks = [tok for tok in vrb_toks if str(tok).isalnum()]\n",
        "\n",
        "    # Appending the json array of subject, relation and object\n",
        "    triples.append({\n",
        "           'subject': sub_toks[0],\n",
        "           'relation': vrb_toks[0],\n",
        "            'object': obj_toks[0]\n",
        "        })\n",
        "print(triples)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'subject': motors, 'relation': pulled, 'object': line}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dzKiWFLbWPA",
        "outputId": "bbd07229-e6f6-44a2-9c54-d639d6bc5a4c"
      },
      "source": [
        "# ----------------- TASK 1 ------------------\n",
        "\n",
        "# Performing Triplets Extraction based on Input Sentence\n",
        "\n",
        "SUBJ = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERB = [\"ROOT\"]\n",
        "OBJ = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
        "text1 = nlp(u\"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.\")\n",
        "str(text1)\n",
        "sentences1 = nltk.sent_tokenize(str(text1))\n",
        "\n",
        "len(sentences1)\n",
        "\n",
        "# Defining triplet empty list and iterating each sentence to find subj tokens,\n",
        "# object tokens and their relations.\n",
        "\n",
        "\n",
        "# Extracting Second text sentence\n",
        "triples1 = []\n",
        "for sentence1 in sentences1:\n",
        "    # break\n",
        "    text1 = nlp(sentence1)\n",
        "    sub_toks1 = [tok for tok in text1 if (tok.dep_ in SUBJ)]\n",
        "    sub_toks1 = [tok for tok in sub_toks1 if str(tok).isalnum()]\n",
        "    obj_toks1 = [tok for tok in text1 if (tok.dep_ in OBJ)]\n",
        "    obj_toks1 = [tok for tok in obj_toks1 if str(tok).isalnum()]\n",
        "    vrb_toks1 = [tok for tok in text1 if (tok.dep_ in VERB)]\n",
        "    vrb_toks1 = [tok for tok in vrb_toks1 if str(tok).isalnum()]\n",
        "\n",
        "    # Appending the json array of subject, relation and object\n",
        "    triples1.append({\n",
        "           'subject': sub_toks1[0],\n",
        "           'relation': vrb_toks1[0],\n",
        "            'object': obj_toks1[0]\n",
        "        })\n",
        "print(triples1)\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'subject': Einstein, 'relation': was, 'object': physicist}, {'subject': He, 'relation': developed, 'object': theory}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO-F9EdM0NvN",
        "outputId": "7f7d90a0-1934-41d6-c67e-d198c4b7cfc9"
      },
      "source": [
        "#------------ TASK 2---------------\n",
        "\n",
        "\n",
        "# Performing Lexical Relations using NLTK Libraries\n",
        "\n",
        "# Defining the empty lists of synonyms,antonyms,hyponyms,hypernyms,meronymns\n",
        "# holonymns and entailment\n",
        "\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "hyponyms = []\n",
        "hypernyms = []\n",
        "meronymns = []\n",
        "holonymns = [] \n",
        "entailment = []\n",
        "\n",
        "# Using the word ship as an example of synset and finding all lexical relations\n",
        "for syn in wn.synsets(\"ship\"):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "# synonyms of word ship\n",
        "print('The synonyms of ship are: ')\n",
        "print(set(synonyms))\n",
        "print('\\n')\n",
        "\n",
        "# antonyms of word ship\n",
        "print('The antonyms of ship are: ')\n",
        "print(set(antonyms))\n",
        "print('\\n')\n",
        "\n",
        "# hyponyms of word ship\n",
        "print('The hyponyms of ship are: ')\n",
        "print(wn.synset('ship.n.01').hyponyms())\n",
        "print('\\n')\n",
        "\n",
        "# hypernyms of word ship\n",
        "print('The hypernyms of ship are: ')\n",
        "print(wn.synset('ship.n.01').hypernyms())\n",
        "print('\\n')\n",
        "\n",
        "# meronymns of word ship\n",
        "print('The meronymns of ship are: ')\n",
        "print(wn.synset('ship.n.01').part_meronyms())\n",
        "print('\\n')\n",
        "\n",
        "# holonymns of word ship\n",
        "print('The holonyms of ship are: ')\n",
        "print(wn.synset('ship.n.01').part_holonyms())\n",
        "print('\\n')\n",
        "\n",
        "# entailment of word ship\n",
        "print('The entailments of ship are: ')\n",
        "print(wn.synset('ship.n.01').entailments())\n",
        "print('\\n')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The synonyms of ship are: \n",
            "{'embark', 'transport', 'ship', 'send'}\n",
            "\n",
            "\n",
            "The antonyms of ship are: \n",
            "{'disembark'}\n",
            "\n",
            "\n",
            "The hyponyms of ship are: \n",
            "[Synset('abandoned_ship.n.01'), Synset('blockade-runner.n.01'), Synset('cargo_ship.n.01'), Synset('flagship.n.02'), Synset('gas-turbine_ship.n.01'), Synset('hospital_ship.n.01'), Synset('hulk.n.02'), Synset('icebreaker.n.01'), Synset('lightship.n.01'), Synset('minelayer.n.01'), Synset('minesweeper.n.01'), Synset('nuclear-powered_ship.n.01'), Synset('passenger_ship.n.01'), Synset('pirate.n.03'), Synset('school_ship.n.01'), Synset('shipwreck.n.01'), Synset('sister_ship.n.01'), Synset('slave_ship.n.01'), Synset('small_ship.n.01'), Synset('steamer.n.03'), Synset('tender.n.06'), Synset('three-decker.n.02'), Synset('transport_ship.n.01'), Synset('treasure_ship.n.01'), Synset('troopship.n.01'), Synset('warship.n.01'), Synset('whaler.n.02'), Synset('wreck.n.04')]\n",
            "\n",
            "\n",
            "The hypernyms of ship are: \n",
            "[Synset('vessel.n.02')]\n",
            "\n",
            "\n",
            "The meronymns of ship are: \n",
            "[Synset('bay.n.04'), Synset('bilge_pump.n.01'), Synset('bilge_well.n.01'), Synset('bulkhead.n.01'), Synset('bulwark.n.02'), Synset('cargo_area.n.01'), Synset('crow's_nest.n.01'), Synset('davit.n.01'), Synset('deck.n.01'), Synset('fin.n.05'), Synset('forecastle.n.01'), Synset('funnel.n.03'), Synset('galley.n.04'), Synset('gyrostabilizer.n.01'), Synset('helm.n.01'), Synset('log.n.05'), Synset('lubber's_hole.n.01'), Synset('porthole.n.01'), Synset('ratline.n.01'), Synset('ridge_rope.n.01'), Synset('riding_bitt.n.01'), Synset('screw.n.03'), Synset('sea_anchor.n.01'), Synset('sheet.n.07'), Synset('skeleton.n.04'), Synset('spar.n.02'), Synset('stern.n.01'), Synset('superstructure.n.01'), Synset('top.n.07'), Synset('topside.n.01'), Synset('winch.n.01')]\n",
            "\n",
            "\n",
            "The holonyms of ship are: \n",
            "[]\n",
            "\n",
            "\n",
            "The entailments of ship are: \n",
            "[]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILHkMJRlhLsc",
        "outputId": "1cab6e03-dbc6-4f9d-9b48-bfc780e21839"
      },
      "source": [
        "# setting the word based on lemma and definition and finding lexical relations\n",
        "wn.synsets('sofa') \n",
        "wn.synsets('ocean') \n",
        "wn.synset('sofa.n.01').definition()\n",
        "wn.synset('sofa.n.01').lemma_names()\n",
        "wn.synset('pasta.n.01').hyponyms()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('cannelloni.n.01'),\n",
              " Synset('lasagna.n.01'),\n",
              " Synset('macaroni_and_cheese.n.01'),\n",
              " Synset('spaghetti.n.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EttW0zWDq0uw",
        "outputId": "9901a651-acbb-4098-a3c3-7836b745bfc2"
      },
      "source": [
        "wn.synset('pasta.n.01').hypernyms()\n",
        "wn.synset('dish.n.02').definition()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a particular item of prepared food'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnTE0N20r3ip",
        "outputId": "b4036242-e54f-4b89-a0f8-2e21159388f0"
      },
      "source": [
        "wn.synsets('ship') \n",
        "wn.synset('ship.n.01').part_holonyms() \n",
        "wn.synset('ship.n.01').part_meronyms() "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('bay.n.04'),\n",
              " Synset('bilge_pump.n.01'),\n",
              " Synset('bilge_well.n.01'),\n",
              " Synset('bulkhead.n.01'),\n",
              " Synset('bulwark.n.02'),\n",
              " Synset('cargo_area.n.01'),\n",
              " Synset('crow's_nest.n.01'),\n",
              " Synset('davit.n.01'),\n",
              " Synset('deck.n.01'),\n",
              " Synset('fin.n.05'),\n",
              " Synset('forecastle.n.01'),\n",
              " Synset('funnel.n.03'),\n",
              " Synset('galley.n.04'),\n",
              " Synset('gyrostabilizer.n.01'),\n",
              " Synset('helm.n.01'),\n",
              " Synset('log.n.05'),\n",
              " Synset('lubber's_hole.n.01'),\n",
              " Synset('porthole.n.01'),\n",
              " Synset('ratline.n.01'),\n",
              " Synset('ridge_rope.n.01'),\n",
              " Synset('riding_bitt.n.01'),\n",
              " Synset('screw.n.03'),\n",
              " Synset('sea_anchor.n.01'),\n",
              " Synset('sheet.n.07'),\n",
              " Synset('skeleton.n.04'),\n",
              " Synset('spar.n.02'),\n",
              " Synset('stern.n.01'),\n",
              " Synset('superstructure.n.01'),\n",
              " Synset('top.n.07'),\n",
              " Synset('topside.n.01'),\n",
              " Synset('winch.n.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1-TqQfzsEXi",
        "outputId": "2bc2ca21-cb1a-4bd9-d10c-14d2d42a88fa"
      },
      "source": [
        "wn.synset('dwelling.n.01').lemma_names()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dwelling', 'home', 'domicile', 'abode', 'habitation', 'dwelling_house']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu5sexLdsJTv",
        "outputId": "940488c5-365f-4fec-bfb1-9863322f55ae"
      },
      "source": [
        " wn.synset('snore.v.01').entailments()\n",
        " wn.synset('buy.v.01').entailments()\n",
        " wn.synset('look.v.01').entailments()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('see.v.01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    }
  ]
}